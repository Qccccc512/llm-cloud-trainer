### 已弃用，现在使用master/templates下的模板
### model
model_name_or_path: {{ model_name }}

### method
stage: sft
do_train: true
finetuning_type: lora
lora_target: all

### dataset
dataset: {{ dataset }}
dataset_dir: /app/data
template: {{ template }}
cutoff_len: 1024

### output
output_dir: /app/output/{{ task_id }}
overwrite_output_dir: true

### train
per_device_train_batch_size: {{ batch_size }}
gradient_accumulation_steps: 4
learning_rate: {{ learning_rate }}
num_train_epochs: {{ epochs }}
logging_steps: 10
save_steps: 100
fp16: true
warmup_ratio: 0.1

### quantization
# quantization_bit: 4
